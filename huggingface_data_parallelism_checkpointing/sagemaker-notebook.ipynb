{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker x Hugging Face Transformers - Distributed Training and Checkpointing Demo\n",
    "### Learn how to use SageMaker distributed training data parallelism and checkpoints in Transformer model fine-tuning.\n",
    "#### Disclaimer: This is a demo showcasing SageMaker Distributed Training data parallelism and checkpointing, but not for direct production use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html) and the [Hugging Face DLCs](https://huggingface.co/docs/sagemaker/main) make it easy to train transformer models using pre-built Hugging Face framework container which supports [SageMaker distributed training](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html). \n",
    "\n",
    "Amazon SageMaker also offers support for [remote S3 Checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html) where data from a local path to Amazon S3 is saved. When the job is restarted, SageMaker copies the data from Amazon S3 back into the local path.\n",
    "\n",
    "In this example, we are going to:\n",
    "\n",
    "- preprocess a dataset in the notebook and upload it to Amazon S3\n",
    "- configure checkpointing and distributed training in the `HuggingFace` estimator\n",
    "- run training and resume training from checkpoints\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine, or Sagemaker Notebook Instances**_ When run in SageMaker Studio, choose kernel `Python 3 (PyTorch 1.10 Python 3.8 CPU Optimized)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Development Environment and Permissions**\n",
    "\n",
    "*Note: we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install \"sagemaker>=2.77.0\" \"transformers==4.12.3\" \"datasets[s3]==1.18.3\" s3fs --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge ipywidgets -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `emotion` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [emotion](https://github.com/dair-ai/emotion_dataset) dataset consists of 16000 training examples, 2000 validation examples, and 2000 testing examples. A more detailed description of the dataset can be found in this [paper](https://aclanthology.org/D18-1404/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model_id used for training and preprocessing\n",
    "model_id = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'emotion'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/emotion'\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset(dataset_name, split=['train', 'test'])\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure checkpointing and distributed training in the `HuggingFace` estimator\n",
    "\n",
    "After we have uploaded dataset, we can configure our SageMaker Estimator parameters to have checkpointing and distributed training enabled. \n",
    "\n",
    "Checkpointing is also used in SageMaker [Managed Spot Training](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html). To configure spot training we need to define the `max_wait` and `max_run` in the `HuggingFace` estimator and set `use_spot_instances` to `True`. In this demo, we are not going to use spot training, thus set `use_spot_instances` to `False`, but feel free to play with it.\n",
    "\n",
    "In spot training:\n",
    "- `max_wait`: Duration in seconds until Amazon SageMaker will stop the managed spot training if not completed yet\n",
    "- `max_run`: Max duration in seconds for training the training job\n",
    "\n",
    "`max_wait` also needs to be greater than `max_run`, because `max_wait` is the duration for waiting/accessing spot instances (can take time when no spot capacity is free) + the expected duration of the training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enables spot training\n",
    "use_spot_instances=False\n",
    "# max time including spot start + training time\n",
    "max_wait=7200\n",
    "# expected training time\n",
    "max_run=4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable checkpointing we need to define `checkpoint_s3_uri` in the `HuggingFace` estimator. `checkpoint_s3_uri` is a S3 URI in which to save the checkpoints. By default Amazon SageMaker will save now any file, which is written to `/opt/ml/checkpoints` in the training job to `checkpoint_s3_uri`. \n",
    "\n",
    "*It is possible to adjust `/opt/ml/checkpoints` by overwriting `checkpoint_local_path` in the `HuggingFace` estimator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 uri where our checkpoints will be uploaded during training\n",
    "base_job_name = \"emotion-checkpointing-1st-job\"\n",
    "\n",
    "checkpoint_s3_uri = f's3://{sess.default_bucket()}/{base_job_name}/checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll enable distributed training to use [SageMaker Distributed Data Parallel Library](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html). \n",
    "\n",
    "The SageMaker distributed data parallel library employs Message Passing Interface (MPI), a popular standard for managing communication between nodes in a high-performance cluster, and uses NVIDIA’s NCCL library for GPU-level communication. You can set custom MPI operations using the `custom_mpi_options parameter` in the `Estimator`. Any `mpirun` flags passed in this field are added to the `mpirun` command and executed by SageMaker for training. \n",
    "\n",
    "For example, you may define the `distribution` parameter of an `Estimator` using the following to use the NCCL_DEBUG variable to print the NCCL version at the start of the program:\n",
    "\n",
    "`distribution = {'smdistributed':{'dataparallel':{'enabled': True, \"custom_mpi_options\": \"-verbose -x NCCL_DEBUG=VERSION\"}}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS has [pre-built framework container images](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) registered with Amazon ECR for SageMaker managed training and inference, and HuggingFace is one of these frameworks that have pre-built docker containers. \n",
    "\n",
    "Next step is to create our `HuggingFace` estimator, provide our `hyperparameters` and add our distributed training and checkpointing configurations. For training instances, choose from `ml.p3.16xlarge`, `ml.p3dn.24xlarge`, and `ml.p4d.24xlarge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'epochs': 2,                       # number of training epochs\n",
    "  'train_batch_size': 32,            # batch size for training\n",
    "  'eval_batch_size': 64,             # batch size for evaluation\n",
    "  'learning_rate': 3e-5,             # learning rate used during training\n",
    "  'model_id':model_id,               # pre-trained model id \n",
    "  'fp16': True,                      # Whether to use 16-bit (mixed) precision training\n",
    "\t'output_dir':'/opt/ml/checkpoints', # make sure files are saved to the checkpoint directory\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train.py',        # fine-tuning script used in training jon\n",
    "    source_dir           = './scripts',       # directory where fine-tuning script is stored\n",
    "    instance_type        = 'ml.p3dn.24xlarge',   # instances type used for the training job\n",
    "    instance_count       = 2,                 # the number of instances used for training\n",
    "    base_job_name        = base_job_name,     # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version = '4.12.3',          # the transformers version used in the training job\n",
    "    pytorch_version      = '1.9.1',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py38',            # the python version used in the training job\n",
    "    hyperparameters      = hyperparameters,   # the hyperparameter used for running the training job\n",
    "\tdistribution         = distribution,\n",
    "    use_spot_instances   = use_spot_instances,# wether to use spot instances or not\n",
    "    # max_wait             = max_wait,          # max time including spot start + training time\n",
    "    # max_run              = max_run,           # max expected training time\n",
    "\tcheckpoint_s3_uri    = checkpoint_s3_uri, # s3 uri where our checkpoints will be uploaded during training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using remote S3 checkpointing you have to make sure that your `train.py` also supports checkpointing. `Transformers` and the `Trainer` offers utilities on how to do this. You only need to add the following snippet to your `Trainer` training script\n",
    "\n",
    "```python\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "# check if checkpoint existing if so continue training\n",
    "if get_last_checkpoint(args.output_dir) is not None:\n",
    "    logger.info(\"***** continue training *****\")\n",
    "    last_checkpoint = get_last_checkpoint(args.output_dir)\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training\n",
    "\n",
    "To start SageMaker training process, we simple call the `.fit` method of our estimator and provide our dataset. After training is finished, you'll see two checkpoints in `checkpoint_s3_uri` bucket, and final model output as a `model.tar.gz` file in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train data object\n",
    "data = {\n",
    "\t'train': training_input_path,\n",
    "    'test': test_input_path\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, logs=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is finished, you'll see two checkpoints in `checkpoint_s3_uri` bucket, and final model output as a `model.tar.gz` file in below s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 uri where the checkpoints are located\n",
    "print(f\"Checkpoint location: \\n{checkpoint_s3_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume training from checkpoints \n",
    "\n",
    "To resume a training job from saved checkpoints, run a new estimator with the same checkpoint_s3_uri that you created in the Enable Checkpointing section. Once the training has resumed, the checkpoints from this S3 bucket are restored to checkpoint_local_path in each instance of the new training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_base_job_name = \"emotion-checkpointing-2nd-job\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'epochs': 3,                       # number of training epochs\n",
    "  'train_batch_size': 32,            # batch size for training\n",
    "  'eval_batch_size': 64,             # batch size for evaluation\n",
    "  'learning_rate': 3e-5,             # learning rate used during training\n",
    "  'model_id':model_id,               # pre-trained model id \n",
    "  'fp16': True,                      # Whether to use 16-bit (mixed) precision training\n",
    "\t'output_dir':'/opt/ml/checkpoints' # make sure files are saved to the checkpoint directory\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator_resume = HuggingFace(\n",
    "    entry_point          = 'train.py',        # fine-tuning script used in training jon\n",
    "    source_dir           = './scripts',       # directory where fine-tuning script is stored\n",
    "    instance_type        = 'ml.p3dn.24xlarge',   # instances type used for the training job\n",
    "    instance_count       = 2,                 # the number of instances used for training\n",
    "    base_job_name        = new_base_job_name,     # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version = '4.12.3',          # the transformers version used in the training job\n",
    "    pytorch_version      = '1.9.1',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py38',            # the python version used in the training job\n",
    "    hyperparameters      = hyperparameters,   # the hyperparameter used for running the training job\n",
    "\tdistribution         = distribution,\n",
    "    use_spot_instances   = use_spot_instances,# wether to use spot instances or not\n",
    "    # max_wait             = max_wait,          # max time including spot start + training time\n",
    "    # max_run              = max_run,           # max expected training time\n",
    "\tcheckpoint_s3_uri    = checkpoint_s3_uri, # s3 uri where previous checkpoints is located\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's kick off the training again. Since we had two checkpoints saved in the first training job, SageMaker starts training from the third epoch. After training is finsihed, you'll be able to see 3 checkpoints in `checkpoint_s3_uri` bucket and a new `model.tar.gz` file in second training job's output bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator_resume.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-1.10-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
