## Table of Contents

### Session 1 Examples

* [data_parallel_pytorch_code_changes](https://github.com/SherryXDing/yahoo-sagemaker-training/tree/main/data_parallel_pytorch_code_changes)
This example points basic code changes when you bring a PyTorch script to use SageMaker distributed training library data parallelism.

* [data_parallel_tensorflow_code_changes](https://github.com/SherryXDing/yahoo-sagemaker-training/tree/main/data_parallel_tensorflow_code_changes)
This example points basic code changes when you bring a Tensorflow script to use SageMaker distributed training library data parallelism.

* [huggingface_data_parallelism_checkpointing](https://github.com/SherryXDing/yahoo-sagemaker-training/tree/main/huggingface_data_parallelism_checkpointing)
This example shows how to leverage SageMaker checkpointing to save transformer checkpoints during SageMaker distribued training (data parallelism) and resume training from checkpoints.

* [huggingface_data_parallelism_incremental_training](https://github.com/SherryXDing/yahoo-sagemaker-training/tree/main/huggingface_data_parallelism_incremental_training)
This example shows how to do SageMaker distributed training (data parallelism) with HuggingFace framework and how to do incremental training from a saved model artifact.

-----

### Session 2 Examples

* [byos_pytorch]()


* [xgboost_script_mode_distributed]()